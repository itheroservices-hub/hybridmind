# üéØ HYBRIDMIND MULTI-MODEL QUICK REFERENCE

## 4 EXECUTION MODES

| Mode | Free Users | Premium Users | Use Case |
|------|------------|---------------|----------|
| **Single** | ‚úÖ 1 model | ‚úÖ 1 model | Quick tasks |
| **Parallel** | ‚úÖ 2 models | ‚úÖ 4 models | Compare outputs |
| **Chain** | ‚úÖ 2 models | ‚úÖ 4 models | Progressive refinement |
| **Agentic** | ‚úÖ 2 models | ‚úÖ 4 models | Autonomous workflows |

---

## üì° API ENDPOINTS

```bash
# Single Model
POST /run/single
{"model": "llama-3.3-70b", "prompt": "Add validation", "code": "..."}

# Parallel (side-by-side)
POST /run/parallel
{"models": ["llama-3.3-70b", "gemini-flash"], "prompt": "Compare", "code": "..."}

# Chaining (sequential)
POST /run/chain
{"models": ["llama-3.3-70b", "gemini-flash"], "prompt": "Improve", "code": "..."}

# Agentic (auto-select models)
POST /agent/plan
{"goal": "Refactor code", "options": {"workflowType": "balanced"}}

# Get model recommendations
POST /models/recommend
{"task": "code-review", "costTier": "medium"}
```

---

## üß† INTELLIGENT PAIRING

### Workflow Strategies

| Strategy | Planner | Executor | Reviewer | Cost/M |
|----------|---------|----------|----------|--------|
| **cost-optimized** | DeepSeek R1 Distill | Llama 3.3 70B | Mistral Small | ~$0.47 |
| **balanced** | Llama 3.3 70B | Claude 3.5 Sonnet | GPT-4o | ~$15.18 |
| **quality-optimized** | OpenAI o1 | Claude Sonnet 4.5 | Claude Opus 4.5 | ~$90 |
| **speed-optimized** | Gemini Flash | Claude Haiku | GPT-4o Mini | ~$1.60 |
| **reasoning-optimized** | DeepSeek R1 | o1-mini | o1 | ~$75 |
| **coding-optimized** | Qwen Coder Flash | Qwen Coder Plus | Codestral | ~$1.60 |

### Task-Based Auto-Selection

```javascript
{
  "code-review": ["claude-opus-4.5", "o1", "claude-sonnet-4.5"],
  "refactoring": ["claude-3.5-sonnet", "qwen3-coder-plus"],
  "optimization": ["deepseek-r1", "o1-mini", "claude-opus-4.5"],
  "debugging": ["qwen3-coder-plus", "gpt-4o", "claude-opus-4.5"],
  "testing": ["gpt-4o-mini", "llama-3.3-70b", "claude-haiku"],
  "documentation": ["claude-3.5-sonnet", "gpt-4o", "mistral-large"],
  "reasoning": ["o1", "deepseek-r1", "o1-mini"]
}
```

---

## üÜì FREE TIER MODELS (6+)

| Model | Strengths | Speed |
|-------|-----------|-------|
| **Llama 3.3 70B** | General coding, reasoning | ‚ö°‚ö°‚ö° |
| **Gemini 2.0 Flash** | Multimodal, fast | ‚ö°‚ö°‚ö° |
| **DeepSeek V3** | Cost-effective reasoning | ‚ö°‚ö° |
| **DeepSeek R1 Distill 70B** | Distilled reasoning | ‚ö°‚ö° |
| **Qwen3 Coder** | Coding specialist | ‚ö°‚ö° |
| **Devstral** | Agentic coding | ‚ö°‚ö° |

---

## ‚≠ê PREMIUM TIER MODELS (25+)

### Reasoning Specialists
- **OpenAI o1** ($15/$60/M) - Ultimate reasoning
- **DeepSeek R1** ($0.55/$2.19/M) - Open-source o1 rival
- **o1-mini** ($3/$12/M) - Fast reasoning

### Coding Specialists
- **Qwen3 Coder Plus** ($0.40/M) - 480B parameters
- **Codestral 2508** ($0.30/$0.90/M) - Mistral's code expert

### General Purpose
- **GPT-4o** ($2.50/$10/M) - Latest OpenAI
- **Claude Sonnet 4.5** ($3/$15/M) - Latest Anthropic
- **Claude Opus 4.5** ($15/$75/M) - Best quality
- **Gemini 2.5 Pro** ($3.50/$10.50/M) - Google flagship

---

## üí° EXAMPLE PAIRINGS

### For Speed (Free)
```json
["llama-3.3-70b", "gemini-flash"]
```

### For Quality (Premium)
```json
["openai/o1", "anthropic/claude-opus-4.5"]
```

### For Coding (Premium)
```json
["qwen/qwen3-coder-plus", "mistralai/codestral-2508"]
```

### For Budget (Free)
```json
["deepseek-v3", "llama-3.3-70b"]
```

### For Balance (Mixed)
```json
["llama-3.3-70b", "anthropic/claude-3.5-sonnet"]
```

### Premium 4-Model Workflow
```json
[
  "openai/o1",                    // Reasoning
  "qwen/qwen3-coder-plus",        // Coding
  "anthropic/claude-opus-4.5",    // Review
  "google/gemini-2.5-pro"         // Alternative perspective
]
```

---

## üîì USER OVERRIDE

**Auto-Selection:**
```bash
POST /agent/plan
{"goal": "Refactor", "options": {"workflowType": "balanced"}}
# System chooses: Llama 3.3 70B ‚Üí Claude 3.5 Sonnet ‚Üí GPT-4o
```

**Manual Override:**
```bash
POST /run/parallel
{"models": ["llama-3.3-70b", "qwen3-coder"]}
# You choose exactly which models
```

---

## üìä TIER COMPARISON

| Feature | Free | Premium |
|---------|------|---------|
| Models | 6+ | 25+ |
| Parallel | 2 | 4 |
| Chain | 2 | 4 |
| Agentic | ‚úÖ | ‚úÖ |
| Auto-select | ‚úÖ | ‚úÖ |
| Override | ‚úÖ | ‚úÖ |
| Reasoning models | ‚ùå | ‚úÖ |
| Coding specialists | ‚ùå | ‚úÖ |

---

## üöÄ QUICK TEST

```bash
# Start backend
cd hybridmind-backend
node server.js

# Test single model
curl -X POST http://localhost:3000/run/single \
  -H "Content-Type: application/json" \
  -d '{"model":"llama-3.3-70b", "prompt":"Add error handling", "code":"function test() { return 1; }"}'

# Test parallel
curl -X POST http://localhost:3000/run/parallel \
  -H "Content-Type: application/json" \
  -d '{"models":["llama-3.3-70b","gemini-flash"], "prompt":"Compare approaches", "code":"function test() { return 1; }"}'

# Test chaining
curl -X POST http://localhost:3000/run/chain \
  -H "Content-Type: application/json" \
  -d '{"models":["llama-3.3-70b","gemini-flash"], "prompt":"Improve progressively", "code":"function test() { return 1; }"}'

# Get models list
curl http://localhost:3000/models

# Get recommendation
curl -X POST http://localhost:3000/models/recommend \
  -H "Content-Type: application/json" \
  -d '{"task":"code-review", "costTier":"medium"}'
```

---

## ‚úÖ SYSTEM STATUS

**ALL 4 MODES: FULLY FUNCTIONAL**

- ‚úÖ Single Model Execution
- ‚úÖ Parallel Comparison (2-4 models)
- ‚úÖ Model Chaining (progressive refinement)
- ‚úÖ Agentic Mode (intelligent auto-pairing)
- ‚úÖ 25+ Models via OpenRouter
- ‚úÖ User Override System
- ‚úÖ Tier-Based Limits
- ‚úÖ Intelligent Selection

**YOU'RE READY TO LAUNCH!** üöÄ
